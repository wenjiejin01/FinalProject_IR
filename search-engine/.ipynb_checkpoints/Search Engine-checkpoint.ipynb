{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## access token informations \n",
    "access_token1 = \"1240651902712516613-BF3CepSxPUoMr3lDkptNt08f2GwaGZ\"\n",
    "access_token_secret1 = \"79ga9EksEgtgpHGJhXy7A4ujb31pw1AjwP8lOlaYtmDdk\"\n",
    "\n",
    "consumer_key1 = \"ieusj8Wof83kmsVe9PDLJQqn3\"\n",
    "consumer_secret1 = \"EpZYsMfEqvHSwFQyAfz2wCdVujPTtr7JQtvkBt1g9sMg5t0m3B\"\n",
    "\n",
    "#Generate API\n",
    "auth = OAuthHandler(consumer_key1, consumer_secret1)\n",
    "auth.set_access_token(access_token1, access_token_secret1)\n",
    "api = API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from output.json scrapped from twitter\n",
    "with open(\"../other-outputs/output.json\", \"rb\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(str_) for str_ in data]\n",
    "\n",
    "df_tweets = pd.DataFrame.from_records(data)\n",
    "\n",
    "# Get unique tweets\n",
    "tweets=df_tweets[df_tweets[\"retweeted_status\"].isna()]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_retweets_uniqueTweets_uniqueUsers(df_tweets):\n",
    "    retweets_ = df_tweets[\"retweeted_status\"].apply(lambda x: 0 if str(x) == \"nan\" else 1)\n",
    "    tot_retweets = sum(retweets_)\n",
    "    unique_tweets = len(retweets_) - tot_retweets\n",
    "    \n",
    "    users_ = df_tweets.user.apply(lambda x: x[\"id\"])\n",
    "    \n",
    "    tot_users = len(set(users_))\n",
    "    \n",
    "    print(\"Total Retweets:\", tot_retweets)\n",
    "\n",
    "    print(\"Unique Tweets:\", unique_tweets)\n",
    "    \n",
    "    print(\"Unique Users:\", tot_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_retweets_uniqueTweets_uniqueUsers(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update (Don't execute this blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(tweets):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Update number of the tweet's likes and retweed.\n",
    "    \"\"\"\n",
    "    for index, tweet in tweets.iterrows():\n",
    "        try:\n",
    "            tweet = api.get_status(tweet[\"id\"])\n",
    "            tweets.loc[index, \"favorite_count\"] = tweet.favorite_count\n",
    "            tweets.loc[index, \"retweet_count\"] = tweet.retweet_count\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"favorite_count\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../other-outputs/tweets.tsv', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Load data from TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../other-outputs/tweets.tsv\", sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Consult the more likes and more retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most liked tweet: When a former VP of Pfizer is calling for the immediate suspension of all SARS CoV 2 vaccine studies, because of th… https://t.co/s6yvRBZKoh, with a total of 171 likes.\n"
     ]
    }
   ],
   "source": [
    "more_likes = 0\n",
    "text_with_more_likes = \"\"\n",
    "for index, tweet in tweets.iterrows():\n",
    "    likes_tweet = tweet[\"favorite_count\"]\n",
    "    if likes_tweet > more_likes:\n",
    "        more_likes = likes_tweet\n",
    "        text_with_more_likes =  tweet[\"text\"]\n",
    "print(\"Most liked tweet: {}, with a total of {} likes.\".format(text_with_more_likes, more_likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most retweets tweet: When a former VP of Pfizer is calling for the immediate suspension of all SARS CoV 2 vaccine studies, because of th… https://t.co/s6yvRBZKoh, with a total of 104 retweets.\n"
     ]
    }
   ],
   "source": [
    "more_retweets = 0\n",
    "text_with_more_retweets = \"\"\n",
    "for index, tweet in tweets.iterrows():\n",
    "    num_retweets = tweet[\"retweet_count\"]\n",
    "    if num_retweets > more_retweets:\n",
    "        more_retweets = num_retweets\n",
    "        text_with_more_retweets = tweet[\"text\"]\n",
    "        \n",
    "print(\"Most retweets tweet: {}, with a total of {} retweets.\".format(text_with_more_retweets, more_retweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTerms(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    tweet -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    tweet - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "        \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    tweet =  tweet.lower() ## Transform in lowercase\n",
    "    tweet = tweet.replace(\"#\", \"\") ## removing the “#” from the word \n",
    "    tweet =  tweet.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    tweet = [word for word in tweet if word not in stops] #Remove stopwords. \n",
    "    tweet =[stemming.stem(word) for word in tweet] \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverted-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets, numTweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    numTweets -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    IdIndex = {} # dictionary to map page titles to page ids\n",
    "    idf=defaultdict(float) # dictionary with inversed document frequency \n",
    "    \n",
    "    for i,tweet in tweets.iterrows():\n",
    "        tweet_id = i\n",
    "        terms = getTerms(tweet[\"text\"]) #page_title + page_text\n",
    "        userid = int(tweet[\"id\"])            \n",
    "        IdIndex[tweet_id]=userid  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current doc and store it in termdictPage\n",
    "        ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ## \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        \n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (termdictPage)\n",
    "                # append the position to the corrisponding list\n",
    "                \n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "        \n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  \n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]+= 1  # increment df for current term   \n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "         # Compute idf\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(numTweets/df[term])),4)                   \n",
    "                    \n",
    "    return index, tf, df, idf, IdIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 43.68 seconds\n"
     ]
    }
   ],
   "source": [
    "#Generate Index and idIndex. \n",
    "start_time = time.time()\n",
    "numTweets = len(tweets)\n",
    "index, tf, df, idf, idIndex = create_index_tfidf(tweets, numTweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_query(ranked_docs): \n",
    "    \"\"\"\n",
    "        This functin is used to print the result. \n",
    "    \"\"\"\n",
    "    top = 20\n",
    "\n",
    "    print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "    resultTmp = []\n",
    "\n",
    "    for d_id in ranked_docs[:top] :\n",
    "        ID = idIndex[d_id]\n",
    "        #print(\"page_id= {} - page_title: {}\".format(d_id, titleIndex[d_id]))\n",
    "        tweetTmp = tweets[tweets[\"id\"] == ID]\n",
    "        tweet = tweetTmp[\"text\"].values[0]\n",
    "        username = ast.literal_eval(tweetTmp[\"user\"].values[0])[\"screen_name\"]\n",
    "        date = tweetTmp[\"created_at\"].values[0]\n",
    "        hashtags_lists = ast.literal_eval(tweetTmp[\"entities\"].values[0])[\"hashtags\"]\n",
    "        hashtags = [tag[\"text\"] for tag in hashtags_lists]\n",
    "        likes = tweetTmp[\"favorite_count\"].values[0]\n",
    "        retweets = tweetTmp[\"retweet_count\"].values[0]\n",
    "        url = ast.literal_eval(tweetTmp[\"user\"].values[0])[\"url\"]\n",
    "\n",
    "        resultTmp.append([tweet,username,date,hashtags,likes,retweets,url])\n",
    "\n",
    "    result = pd.DataFrame(resultTmp,columns=[\"Tweet\", \"Username\", \"Date\", \"Hashtags\", \"Likes\", \"Retweets\", \"Url\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ranking Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(terms, docs, index, idf, tf, idIndex):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    idIndex -- mapping between page id and tweet id\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "                                                                                                                                                                                                                                                                     \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            if doc in docs:\n",
    "                tweetInfo = tweets[tweets[\"id\"] == idIndex[doc]]\n",
    "                like =  tweetInfo[\"favorite_count\"].values\n",
    "                retweet = tweetInfo[\"retweet_count\"].values\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ idIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search with tf-idf ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, idIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "covid-19 america\n",
      "\n",
      "======================\n",
      "Top 20 results out of 155 for the seached query:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lock them all up they are responsible for the ...</td>\n",
       "      <td>madog190157</td>\n",
       "      <td>Sat Dec 05 10:13:18 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@WillyMasterson @JoeBiden Everyone in America ...</td>\n",
       "      <td>Johnsontb1</td>\n",
       "      <td>Sat Dec 05 10:22:01 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reading Covid discourse from America on Twitte...</td>\n",
       "      <td>JamieCPitman</td>\n",
       "      <td>Sat Dec 05 12:29:49 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://Instagram.com/jcpitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@chris_herd I’m witnessing it now. My concern ...</td>\n",
       "      <td>NakoMbelle</td>\n",
       "      <td>Sat Dec 05 10:21:15 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.fintechrecruiters.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After shed loads of information warnings &amp;amp;...</td>\n",
       "      <td>alanjoh1949</td>\n",
       "      <td>Sat Dec 05 10:11:10 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@lack__lustre Covid-19</td>\n",
       "      <td>RanaTal80613033</td>\n",
       "      <td>Sat Dec 05 10:21:33 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What have you experienced with this Covid-19 p...</td>\n",
       "      <td>misstmoraa</td>\n",
       "      <td>Sat Dec 05 10:20:54 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As well did COVID-19 Deaths https://t.co/kX8Hw...</td>\n",
       "      <td>ParodyTheodore</td>\n",
       "      <td>Sat Dec 05 10:22:04 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Have Covid-19 passport, will travel https://t....</td>\n",
       "      <td>BusinessTimes</td>\n",
       "      <td>Sat Dec 05 10:21:13 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>COVID-19 Malaysia Updates https://t.co/9aSK7l1lYz</td>\n",
       "      <td>HowKokKeng1</td>\n",
       "      <td>Sat Dec 05 10:13:18 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Asymptomatic People Do NOT Spread COVID-19 htt...</td>\n",
       "      <td>Vman02605333</td>\n",
       "      <td>Sat Dec 05 10:13:32 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.magapill.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@AngieJo63181310 @JoeBiden The White House is ...</td>\n",
       "      <td>Chanucka2</td>\n",
       "      <td>Sat Dec 05 12:29:44 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Telehealth usage soars during COVID-19 https:/...</td>\n",
       "      <td>BackboneUK</td>\n",
       "      <td>Sat Dec 05 10:13:33 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.backbone.uk.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@anilvijminister Covid-19 is not pendamic it's...</td>\n",
       "      <td>UttamCh35735268</td>\n",
       "      <td>Sat Dec 05 10:11:36 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>United States faces dramatic worsening of Covi...</td>\n",
       "      <td>nobodytweetnob</td>\n",
       "      <td>Sat Dec 05 10:13:40 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#LatestNews #coronavirus Covid-19 &amp;amp; Mortga...</td>\n",
       "      <td>mortgage_guru</td>\n",
       "      <td>Sat Dec 05 10:13:32 +0000 2020</td>\n",
       "      <td>[LatestNews, coronavirus]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Pakistan records over 3,000 Covid-19 cases for...</td>\n",
       "      <td>MouliNalagatla</td>\n",
       "      <td>Sat Dec 05 10:12:14 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Petition: Prevent any restrictions on those wh...</td>\n",
       "      <td>fsprops96</td>\n",
       "      <td>Sat Dec 05 10:11:11 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.fsproperties.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Moscow Starts Mass COVID-19 Vaccination With I...</td>\n",
       "      <td>Dailynews4meCom</td>\n",
       "      <td>Sat Dec 05 10:22:09 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://dailynews4me.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@GavinWax @NYYRC This is the party where Covid...</td>\n",
       "      <td>Rez_AmongUs</td>\n",
       "      <td>Sat Dec 05 10:21:22 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet         Username  \\\n",
       "0   Lock them all up they are responsible for the ...      madog190157   \n",
       "1   @WillyMasterson @JoeBiden Everyone in America ...       Johnsontb1   \n",
       "2   Reading Covid discourse from America on Twitte...     JamieCPitman   \n",
       "3   @chris_herd I’m witnessing it now. My concern ...       NakoMbelle   \n",
       "4   After shed loads of information warnings &amp;...      alanjoh1949   \n",
       "5                              @lack__lustre Covid-19  RanaTal80613033   \n",
       "6   What have you experienced with this Covid-19 p...       misstmoraa   \n",
       "7   As well did COVID-19 Deaths https://t.co/kX8Hw...   ParodyTheodore   \n",
       "8   Have Covid-19 passport, will travel https://t....    BusinessTimes   \n",
       "9   COVID-19 Malaysia Updates https://t.co/9aSK7l1lYz      HowKokKeng1   \n",
       "10  Asymptomatic People Do NOT Spread COVID-19 htt...     Vman02605333   \n",
       "11  @AngieJo63181310 @JoeBiden The White House is ...        Chanucka2   \n",
       "12  Telehealth usage soars during COVID-19 https:/...       BackboneUK   \n",
       "13  @anilvijminister Covid-19 is not pendamic it's...  UttamCh35735268   \n",
       "14  United States faces dramatic worsening of Covi...   nobodytweetnob   \n",
       "15  #LatestNews #coronavirus Covid-19 &amp; Mortga...    mortgage_guru   \n",
       "16  Pakistan records over 3,000 Covid-19 cases for...   MouliNalagatla   \n",
       "17  Petition: Prevent any restrictions on those wh...        fsprops96   \n",
       "18  Moscow Starts Mass COVID-19 Vaccination With I...  Dailynews4meCom   \n",
       "19  @GavinWax @NYYRC This is the party where Covid...      Rez_AmongUs   \n",
       "\n",
       "                              Date                   Hashtags  Likes  \\\n",
       "0   Sat Dec 05 10:13:18 +0000 2020                         []      1   \n",
       "1   Sat Dec 05 10:22:01 +0000 2020                         []      0   \n",
       "2   Sat Dec 05 12:29:49 +0000 2020                         []      0   \n",
       "3   Sat Dec 05 10:21:15 +0000 2020                         []      1   \n",
       "4   Sat Dec 05 10:11:10 +0000 2020                         []      4   \n",
       "5   Sat Dec 05 10:21:33 +0000 2020                         []      1   \n",
       "6   Sat Dec 05 10:20:54 +0000 2020                         []      0   \n",
       "7   Sat Dec 05 10:22:04 +0000 2020                         []      3   \n",
       "8   Sat Dec 05 10:21:13 +0000 2020                         []      0   \n",
       "9   Sat Dec 05 10:13:18 +0000 2020                         []      0   \n",
       "10  Sat Dec 05 10:13:32 +0000 2020                         []      0   \n",
       "11  Sat Dec 05 12:29:44 +0000 2020                         []      1   \n",
       "12  Sat Dec 05 10:13:33 +0000 2020                         []      0   \n",
       "13  Sat Dec 05 10:11:36 +0000 2020                         []      0   \n",
       "14  Sat Dec 05 10:13:40 +0000 2020                         []      0   \n",
       "15  Sat Dec 05 10:13:32 +0000 2020  [LatestNews, coronavirus]      0   \n",
       "16  Sat Dec 05 10:12:14 +0000 2020                         []      0   \n",
       "17  Sat Dec 05 10:11:11 +0000 2020                         []      0   \n",
       "18  Sat Dec 05 10:22:09 +0000 2020                         []      0   \n",
       "19  Sat Dec 05 10:21:22 +0000 2020                         []      0   \n",
       "\n",
       "    Retweets                               Url  \n",
       "0          0                              None  \n",
       "1          0                              None  \n",
       "2          0     http://Instagram.com/jcpitman  \n",
       "3          0  http://www.fintechrecruiters.com  \n",
       "4          1                              None  \n",
       "5          0                              None  \n",
       "6          0                              None  \n",
       "7          0                              None  \n",
       "8          0                              None  \n",
       "9          0                              None  \n",
       "10         0          http://www.magapill.com/  \n",
       "11         0                              None  \n",
       "12         1        http://www.backbone.uk.com  \n",
       "13         0                              None  \n",
       "14         0                              None  \n",
       "15         0                              None  \n",
       "16         0                              None  \n",
       "17         0     http://www.fsproperties.co.uk  \n",
       "18         0         https://dailynews4me.com/  \n",
       "19         0                              None  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "\n",
    "output_query(ranked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocumentsWithLikes(terms, docs, index, idf, tf, idIndex):\n",
    "    \"\"\"\n",
    "    In this function we compute the out ranking score with like and retweets counts. \n",
    "    Because in this case, the amount of rwteets and the number of user's likes\n",
    "    are also one of the important factors for ranking. \n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    idIndex -- mapping between page id and tweet id\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "                                                                                                                                                                                                                                                                     \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            if doc in docs:\n",
    "                tweetInfo = tweets[tweets[\"id\"] == idIndex[doc]]\n",
    "                like =  tweetInfo[\"favorite_count\"].values\n",
    "                retweet = tweetInfo[\"retweet_count\"].values\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term] + (like + retweet)\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(np.squeeze(curDocVec), np.squeeze(queryVector)), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ idIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### My-Score + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_score(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocumentsWithLikes(query, docs, index, idf, tf, idIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "vaccine covid19\n",
      "defaultdict(<function rankDocumentsWithLikes.<locals>.<lambda> at 0x000001F1AA232280>, {51: [array([0.63015498]), 0], 55: [array([0.7533162]), 0], 117: [array([0.2550912]), 0], 125: [array([0.70468944]), array([1.16168208])], 136: [array([11.60085935]), 0], 138: [array([0.55283046]), 0], 166: [array([0.70468944]), 0], 177: [array([0.63015498]), 0], 180: [array([0.66423357]), 0], 181: [array([1.498225]), array([1.821325])], 207: [array([0.70468944]), 0], 241: [array([0.55283046]), 0], 256: [array([0.55283046]), 0], 326: [array([0.66423357]), 0], 335: [array([0.57535023]), 0], 358: [array([0.53270217]), 0], 366: [array([1.60085935]), 0], 372: [array([31.23017995]), 0], 395: [array([0.55283046]), 0], 434: [array([0.70468944]), 0], 436: [array([0.7533162]), 0], 456: [array([1.55283046]), 0], 461: [array([0.57535023]), 0], 466: [array([0.70468944]), 0], 472: [array([0.70468944]), 0], 485: [array([0.498225]), 0], 503: [array([0.55283046]), 0], 507: [array([0.57535023]), 0], 509: [array([0.63015498]), 0], 533: [array([2.63015498]), 0], 539: [array([0.63015498]), 0], 581: [array([0.81350178]), array([1.34105946])], 582: [array([0.70468944]), 0], 588: [array([0.57535023]), 0], 589: [array([0.55283046]), 0], 600: [array([1.55283046]), 0], 618: [array([0.89122488]), array([1.46918616])], 625: [array([0.70468944]), 0], 639: [array([1.63015498]), 0], 651: [array([0.81350178]), 0], 655: [array([2.55283046]), 0], 739: [array([1.57535023]), 0], 743: [array([0.66423357]), array([1.09499049])], 758: [array([0.51456678]), 0], 778: [array([0.70468944]), 0], 780: [array([0.57535023]), 0], 822: [array([0.60085935]), 0], 922: [array([0.66423357]), 0], 946: [array([0.55283046]), 0], 952: [array([3.53270217]), 0], 1002: [array([4.51456678]), 0], 1044: [array([0.57535023]), 0], 1052: [array([0.66423357]), 0], 1140: [array([12.53270217]), 0], 1165: [array([4.51456678]), 0], 1166: [array([0.63015498]), 0], 1196: [array([93.99645]), 0], 1227: [array([0.57535023]), 0], 1231: [array([6.55283046]), 0], 1246: [array([0.63015498]), 0], 1247: [array([83.93945306]), 0], 1254: [array([7.55283046]), 0], 1264: [array([1.81350178]), 0], 1315: [array([0.63015498]), 0], 1323: [array([0.70468944]), 0], 1344: [array([0.55283046]), 0], 1358: [array([0.57535023]), 0], 1384: [array([2.57535023]), 0], 1389: [array([0.60085935]), 0], 1390: [array([0.57535023]), 0], 1423: [array([0.55283046]), 0], 1457: [array([0.81350178]), 0], 1463: [array([1.10546163]), 0], 1468: [array([0.60085935]), 0], 1490: [array([0.57535023]), 0], 1564: [array([0.81350178]), 0], 1570: [array([0.57535023]), 0], 1636: [array([0.57535023]), 0], 1648: [array([0.498225]), 0], 1653: [array([0.60085935]), 0], 1666: [array([0.57535023]), 0], 1668: [array([0.55283046]), 0], 1682: [array([0.498225]), 0], 1712: [array([0.63015498]), 0], 1724: [array([0.57535023]), 0], 1737: [array([0.53270217]), 0], 1741: [array([0.63015498]), 0], 1763: [array([1.45717126]), 0], 1768: [array([2.57535023]), 0], 1772: [array([0.53270217]), 0], 1792: [array([5.55283046]), 0], 1826: [array([0.63015498]), 0], 1830: [array([0.70468944]), 0], 1831: [array([0.498225]), 0], 1841: [array([0.53270217]), 0], 1860: [array([2.02913356]), array([1.84826446])], 1886: [array([0.81350178]), 0], 1888: [array([0.66423357]), 0], 1898: [array([0.63015498]), 0], 1906: [array([0.70468944]), 0], 1907: [array([0.66423357]), 0], 1939: [array([1.70468944]), 0], 1940: [array([0.55283046]), 0], 1941: [array([8.55283046]), 0], 2011: [array([3.53270217]), 0], 2043: [array([0.70468944]), 0], 2097: [array([0.81350178]), 0], 2099: [array([0.7533162]), array([1.2418434])], 2124: [array([2.99645]), 0], 2216: [array([0.45717126]), 0], 2259: [array([0.57535023]), 0], 2271: [array([0.7533162]), 0], 2366: [array([16.55283046]), 0], 2373: [array([0.57535023]), 0], 2393: [array([0.60085935]), 0], 2430: [array([0.46972653]), 0], 2445: [array([0.55283046]), 0], 2447: [array([0.57535023]), 0], 2452: [array([29.57535023]), 0], 2458: [array([0.70468944]), 0], 2477: [array([0.53270217]), 0], 2482: [array([22.70468944]), 0], 2503: [array([0.53270217]), 0], 2515: [array([1.498225]), 0], 2531: [array([0.7533162]), array([1.2418434])], 2582: [array([1.57535023]), array([1.94846611])], 2671: [array([0.60085935]), 0], 2678: [array([16.57535023]), 0], 2682: [array([0.57535023]), 0], 2688: [array([4.57535023]), 0], 2700: [array([0.60085935]), 0], 2760: [array([0.48327825]), 0], 2764: [array([0.63015498]), 0], 2860: [array([0.498225]), 0], 2916: [array([1.7533162]), 0], 2920: [array([0.51456678]), 0], 2921: [array([0.81350178]), 0], 2923: [array([0.57535023]), 0], 2924: [array([6.57535023]), 0], 3001: [array([0.55283046]), 0], 3008: [array([13.57535023]), 0], 3013: [array([3.57535023]), 0], 3024: [array([5.53270217]), 0], 3052: [array([3.60085935]), 0], 3059: [array([1.06520505]), 0], 3083: [array([1.70468944]), 0], 3088: [array([4.55283046]), 0], 3117: [array([0.70468944]), 0], 3131: [array([0.66423357]), 0], 3138: [array([5.60085935]), 0], 3164: [array([1.7533162]), 0], 3166: [array([0.498225]), 0], 3175: [array([0.57535023]), 0], 3206: [array([1.10546163]), 0], 3235: [array([0.89122488]), 0], 3267: [array([0.66423357]), array([1.09499049])], 3291: [array([1.57535023]), 0], 3306: [array([0.55283046]), 0], 3314: [array([275.55283046]), 0], 3325: [array([0.53270217]), 0], 3362: [array([0.70468944]), 0], 3368: [array([1.63015498]), 0], 3384: [array([22.57535023]), 0], 3391: [array([1.55283046]), 0], 3430: [array([2.06520505]), 0], 3468: [array([2.44561244]), 0], 3493: [array([3.55283046]), 0], 3521: [array([0.7533162]), 0], 3561: [array([0.51456678]), 0], 3612: [array([0.55283046]), 0], 3614: [array([3.57535023]), 0], 3642: [array([17.63015498]), 0], 3690: [array([20.60085935]), array([20.99051795])], 3694: [array([9.48327825]), 0], 3697: [array([0.70468944]), 0], 3709: [array([1.66423357]), 0], 3712: [array([2.2017187]), 0], 3735: [array([1.63015498]), 0], 3799: [array([1.53270217]), 0], 3829: [array([0.81350178]), 0], 3866: [array([0.57535023]), 0], 3870: [array([1.02913356]), 0], 3892: [array([2.55283046]), 0], 3895: [array([0.60085935]), 0], 3992: [array([0.60085935]), 0], 4031: [array([6.63015498]), 0], 4033: [array([0.51456678]), 0], 4086: [array([0.66423357]), 0], 4137: [array([1.53270217]), 0], 4149: [array([0.57535023]), 0], 4158: [array([0.63015498]), 0], 4184: [array([1.70468944]), 0], 4188: [array([0.53270217]), 0], 318: [0, array([0.91134222])], 367: [0, array([42.94846611])], 451: [0, array([2.91134222])], 562: [0, array([0.91134222])], 638: [0, array([4.94846611])], 816: [0, array([0.94846611])], 860: [0, array([1.03881186])], 977: [0, array([48.03881186])], 1149: [0, array([1.87816069])], 1368: [0, array([0.94846611])], 1535: [0, array([1.99051795])], 1545: [0, array([0.91134222])], 1918: [0, array([7.91134222])], 1929: [0, array([0.79668525])], 2032: [0, array([2.03881186])], 2152: [0, array([0.79668525])], 2224: [0, array([1.87816069])], 2289: [0, array([2.9810359])], 2360: [0, array([0.84826446])], 2390: [0, array([1.03881186])], 2455: [0, array([0.84826446])], 2457: [0, array([0.94846611])], 2580: [0, array([0.99051795])], 2631: [0, array([4.2418434])], 2662: [0, array([2.09499049])], 2686: [0, array([0.77434521])], 2843: [0, array([1.84826446])], 2877: [0, array([2.91134222])], 2880: [0, array([0.79668525])], 2918: [0, array([0.91134222])], 3010: [0, array([0.94846611])], 3393: [0, array([1.99051795])], 3483: [0, array([1.87816069])], 3550: [0, array([4.87816069])], 3574: [0, array([0.94846611])], 3652: [0, array([3.09499049])], 3831: [0, array([0.99051795])], 3921: [0, array([2.91134222])], 3927: [0, array([1.46918616])], 4098: [0, array([25.821325])], 4141: [0, array([1.03881186])], 4143: [0, array([0.94846611])]})\n",
      "[1.4091931042266703, 2.3230579082321645]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-67317cfeaa5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Insert your query:\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mranked_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0moutput_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mranked_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-b4da9c71e717>\u001b[0m in \u001b[0;36mmy_score\u001b[1;34m(query, index)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdocs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mranked_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrankDocumentsWithLikes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mranked_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-194600f81fd6>\u001b[0m in \u001b[0;36mrankDocumentsWithLikes\u001b[1;34m(terms, docs, index, idf, tf, idIndex)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocVectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mdocScores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurDocVec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueryVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurDocVec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mdocScores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mresultDocs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-194600f81fd6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocVectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mdocScores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurDocVec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueryVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurDocVec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mdocScores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mresultDocs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,1) and (2,) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = my_score(query, index)    \n",
    "output_query(ranked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docs, preprocess=preprocess_string, verbose=10000):\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        yield preprocess(doc)#  preprocess\n",
    "        \n",
    "        # print progress if needed\n",
    "        if verbose > 0 and (i + 1) % verbose == 0:\n",
    "            print(f\"Progress: {i + 1}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = {}\n",
    "for text in df_tweets[\"text\"]:\n",
    "    \n",
    "    # remove \"RT\" string indicating a retweet\n",
    "    text = text.replace(\"RT \", \"\").strip()\n",
    "    \n",
    "    # lowering text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing all the punctuations\n",
    "    text = re.sub(r'[^\\w\\s]','',text).strip()\n",
    "    \n",
    "    # tokenize the text\n",
    "    lst_text = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    lst_text = [x for x in lst_text if x not in STOPWORDS]\n",
    "    \n",
    "        \n",
    "    # create bag-of-words - for each word the frequency of the word in the corpus\n",
    "    for w in lst_text:\n",
    "        if w not in bag_of_words:\n",
    "            bag_of_words[w] = 0\n",
    "        bag_of_words[w] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(title, dic_):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(18,7))\n",
    "    wordcloud = WordCloud(background_color=\"white\",width=1600, height=800)\n",
    "    wordcloud = wordcloud.generate_from_frequencies(dic_)\n",
    "    ax.axis(\"off\")     \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(\"WordCloud - All Tweets\", bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot of the 10 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    hashtags_lists = df_tweets[\"entities\"].apply(lambda x: x[\"hashtags\"])\n",
    "    \n",
    "    hashtags = hashtags_lists.apply(lambda x: x[0][\"text\"].lower() if x != [] else None)\n",
    "    \n",
    "    hashtags_by_frequency = Counter(hashtags)\n",
    "    hashtags_by_frequency = {k: hashtags_by_frequency[k] for k in hashtags_by_frequency if k != None} \n",
    "    \n",
    "    return hashtags_by_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_by_frequency = extract_hashtags()\n",
    "hashtags_by_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags = pd.DataFrame(hashtags_by_frequency.items())\n",
    "df_hashtags.columns = [\"hashtag\", \"count\"]\n",
    "df_hashtags.set_index(\"hashtag\", inplace=True)\n",
    "df_hashtags.sort_values(\"count\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags.head(10).plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
