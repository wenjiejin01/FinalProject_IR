{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import json\n",
    "from numpy import linalg as la\n",
    "import ast\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## access token informations \n",
    "access_token1 = \"1240651902712516613-BF3CepSxPUoMr3lDkptNt08f2GwaGZ\"\n",
    "access_token_secret1 = \"79ga9EksEgtgpHGJhXy7A4ujb31pw1AjwP8lOlaYtmDdk\"\n",
    "\n",
    "consumer_key1 = \"ieusj8Wof83kmsVe9PDLJQqn3\"\n",
    "consumer_secret1 = \"EpZYsMfEqvHSwFQyAfz2wCdVujPTtr7JQtvkBt1g9sMg5t0m3B\"\n",
    "\n",
    "#Generate API\n",
    "auth = OAuthHandler(consumer_key1, consumer_secret1)\n",
    "auth.set_access_token(access_token1, access_token_secret1)\n",
    "api = API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from output.json scrapped from twitter\n",
    "with open(\"../other-outputs/output.json\", \"rb\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(str_) for str_ in data]\n",
    "\n",
    "df_tweets = pd.DataFrame.from_records(data)\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update (Don't execute this blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(tweets):\n",
    "    \"\"\"\n",
    "    Update number of the tweet's likes and retweed.\n",
    "    \"\"\"\n",
    "    for index, tweet in tweets.iterrows():\n",
    "        try:\n",
    "            tweet = api.get_status(tweet[\"id\"])\n",
    "            tweets.loc[index, \"favorite_count\"] = tweet.favorite_count\n",
    "            tweets.loc[index, \"retweet_count\"] = tweet.retweet_count\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 564\n"
     ]
    }
   ],
   "source": [
    "update(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0\n",
       "2     1\n",
       "5     0\n",
       "13    1\n",
       "16    1\n",
       "24    0\n",
       "Name: favorite_count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"favorite_count\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../other-outputs/tweets.tsv', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Load data from TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../other-outputs/tweets.tsv\", sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Consult the more likes and more retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most liked tweet: When a former VP of Pfizer is calling for the immediate suspension of all SARS CoV 2 vaccine studies, because of th… https://t.co/s6yvRBZKoh, with a total of 835 likes.\n"
     ]
    }
   ],
   "source": [
    "more_likes = 0\n",
    "text_with_more_likes = \"\"\n",
    "for index, tweet in tweets.iterrows():\n",
    "    likes_tweet = tweet[\"favorite_count\"]\n",
    "    if likes_tweet > more_likes:\n",
    "        more_likes = likes_tweet\n",
    "        text_with_more_likes =  tweet[\"text\"]\n",
    "print(\"Most liked tweet: {}, with a total of {} likes.\".format(text_with_more_likes, more_likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most retweets tweet: When a former VP of Pfizer is calling for the immediate suspension of all SARS CoV 2 vaccine studies, because of th… https://t.co/s6yvRBZKoh, with a total of 486 retweets.\n"
     ]
    }
   ],
   "source": [
    "more_retweets = 0\n",
    "text_with_more_retweets = \"\"\n",
    "for index, tweet in tweets.iterrows():\n",
    "    num_retweets = tweet[\"retweet_count\"]\n",
    "    if num_retweets > more_retweets:\n",
    "        more_retweets = num_retweets\n",
    "        text_with_more_retweets = tweet[\"text\"]\n",
    "        \n",
    "print(\"Most retweets tweet: {}, with a total of {} retweets.\".format(text_with_more_retweets, more_retweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTerms(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    tweet -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    tweet - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "        \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    tweet =  tweet.lower() ## Transform in lowercase\n",
    "    tweet = tweet.replace(\"#\", \"\") ## removing the “#” from the word \n",
    "    tweet =  tweet.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    tweet = [word for word in tweet if word not in stops] #Remove stopwords. \n",
    "    tweet =[stemming.stem(word) for word in tweet] \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverted-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets, numTweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    numTweets -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    IdIndex = {} # dictionary to map page titles to page ids\n",
    "    idf=defaultdict(float) # dictionary with inversed document frequency \n",
    "    \n",
    "    for i,tweet in tweets.iterrows():\n",
    "        tweet_id = i\n",
    "        terms = getTerms(tweet[\"text\"]) #page_title + page_text\n",
    "        userid = int(tweet[\"id\"])            \n",
    "        IdIndex[tweet_id]=userid  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current doc and store it in termdictPage\n",
    "        ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ## \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        \n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (termdictPage)\n",
    "                # append the position to the corrisponding list\n",
    "                \n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "        \n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  \n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]+= 1  # increment df for current term   \n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "         # Compute idf\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(numTweets/df[term])),4)                   \n",
    "                    \n",
    "    return index, tf, df, idf, IdIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 40.73 seconds\n"
     ]
    }
   ],
   "source": [
    "#Generate Index and idIndex. \n",
    "start_time = time.time()\n",
    "numTweets = len(tweets)\n",
    "index, tf, df, idf, idIndex = create_index_tfidf(tweets, numTweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ranking Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(terms, docs, index, idf, tf, idIndex):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    idIndex -- mapping between page id and tweet id\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "                                                                                                                                                                                                                                                                     \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            if doc in docs:\n",
    "                tweetInfo = tweets[tweets[\"id\"] == idIndex[doc]]\n",
    "                like =  tweetInfo[\"favorite_count\"].values\n",
    "                retweet = tweetInfo[\"retweet_count\"].values\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ idIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocumentsWithLikes(terms, docs, index, idf, tf, idIndex):\n",
    "    \"\"\"\n",
    "    In this function we compute the out ranking score with like and retweets counts. \n",
    "    Because in this case, the amount of rwteets and the number of user's likes\n",
    "    are also one of the important factors for ranking. \n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    idIndex -- mapping between page id and tweet id\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "                                                                                                                                                                                                                                                                     \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            if doc in docs:\n",
    "                tweetInfo = tweets[tweets[\"id\"] == idIndex[doc]]\n",
    "                like =  tweetInfo[\"favorite_count\"].values\n",
    "                retweet = tweetInfo[\"retweet_count\"].values\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term] + (like + retweet)\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ idIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search with tf-idf ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, idIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### My-Score + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocumentsWithLikes(query, docs, index, idf, tf, idIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "covidÇ\n",
      "No results found, try again\n",
      "covid\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 20 results out of 0 for the seached query:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Tweet, Username, Date, Hashtags, Likes, Retweets, Url]\n",
       "Index: []"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "resultTmp = []\n",
    "\n",
    "for d_id in ranked_docs[:top] :\n",
    "    ID = idIndex[d_id]\n",
    "    #print(\"page_id= {} - page_title: {}\".format(d_id, titleIndex[d_id]))\n",
    "    tweetTmp = tweets[tweets[\"id\"] == ID]\n",
    "    tweet = tweetTmp[\"text\"].values[0]\n",
    "    username = ast.literal_eval(tweetTmp[\"user\"].values[0])[\"screen_name\"]\n",
    "    date = tweetTmp[\"created_at\"].values[0]\n",
    "    hashtags_lists = ast.literal_eval(tweetTmp[\"entities\"].values[0])[\"hashtags\"]\n",
    "    hashtags = [tag[\"text\"] for tag in hashtags_lists]\n",
    "    likes = tweetTmp[\"favorite_count\"].values[0]\n",
    "    retweets = tweetTmp[\"retweet_count\"].values[0]\n",
    "    url = ast.literal_eval(tweetTmp[\"user\"].values[0])[\"url\"]\n",
    "    \n",
    "    resultTmp.append([tweet,username,date,hashtags,likes,retweets,url])\n",
    "    \n",
    "result = pd.DataFrame(resultTmp,columns=[\"Tweet\", \"Username\", \"Date\", \"Hashtags\", \"Likes\", \"Retweets\", \"Url\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
