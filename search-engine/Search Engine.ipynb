{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import json\n",
    "from numpy import linalg as la\n",
    "import ast\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## access token informations \n",
    "access_token1 = \"1240651902712516613-BF3CepSxPUoMr3lDkptNt08f2GwaGZ\"\n",
    "access_token_secret1 = \"79ga9EksEgtgpHGJhXy7A4ujb31pw1AjwP8lOlaYtmDdk\"\n",
    "\n",
    "consumer_key1 = \"ieusj8Wof83kmsVe9PDLJQqn3\"\n",
    "consumer_secret1 = \"EpZYsMfEqvHSwFQyAfz2wCdVujPTtr7JQtvkBt1g9sMg5t0m3B\"\n",
    "\n",
    "#Generate API\n",
    "auth = OAuthHandler(consumer_key1, consumer_secret1)\n",
    "auth.set_access_token(access_token1, access_token_secret1)\n",
    "api = API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>lang</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>extended_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat Dec 05 10:11:06 +0000 2020</td>\n",
       "      <td>1335164788498980864</td>\n",
       "      <td>1335164788498980864</td>\n",
       "      <td>RT @JoeDanMedia: In 2009... Santelli lit the T...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>1607163066397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Dec 05 10:11:06 +0000 2020</td>\n",
       "      <td>1335164788767277061</td>\n",
       "      <td>1335164788767277061</td>\n",
       "      <td>RT @CNNPolitics: President-elect Joe Biden on ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>1607163066461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat Dec 05 10:11:06 +0000 2020</td>\n",
       "      <td>1335164789060984832</td>\n",
       "      <td>1335164789060984832</td>\n",
       "      <td>Arbroath #arbroath #covid</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>1607163066531</td>\n",
       "      <td>1.335162e+18</td>\n",
       "      <td>1335162235216420864</td>\n",
       "      <td>{'created_at': 'Sat Dec 05 10:00:57 +0000 2020...</td>\n",
       "      <td>{'url': 'https://t.co/HrzZRGUtCS', 'expanded':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat Dec 05 10:11:06 +0000 2020</td>\n",
       "      <td>1335164790180864001</td>\n",
       "      <td>1335164790180864001</td>\n",
       "      <td>RT @johnnymorls: I think that wraps things up ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>1607163066798</td>\n",
       "      <td>1.335132e+18</td>\n",
       "      <td>1335132252800503808</td>\n",
       "      <td>{'created_at': 'Sat Dec 05 08:01:49 +0000 2020...</td>\n",
       "      <td>{'url': 'https://t.co/738ZoPEBv7', 'expanded':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat Dec 05 10:11:06 +0000 2020</td>\n",
       "      <td>1335164790365368320</td>\n",
       "      <td>1335164790365368320</td>\n",
       "      <td>RT @CAOYI170610: According to a recent US CDC ...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>1607163066842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at                   id               id_str  \\\n",
       "0  Sat Dec 05 10:11:06 +0000 2020  1335164788498980864  1335164788498980864   \n",
       "1  Sat Dec 05 10:11:06 +0000 2020  1335164788767277061  1335164788767277061   \n",
       "2  Sat Dec 05 10:11:06 +0000 2020  1335164789060984832  1335164789060984832   \n",
       "3  Sat Dec 05 10:11:06 +0000 2020  1335164790180864001  1335164790180864001   \n",
       "4  Sat Dec 05 10:11:06 +0000 2020  1335164790365368320  1335164790365368320   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @JoeDanMedia: In 2009... Santelli lit the T...   \n",
       "1  RT @CNNPolitics: President-elect Joe Biden on ...   \n",
       "2                          Arbroath #arbroath #covid   \n",
       "3  RT @johnnymorls: I think that wraps things up ...   \n",
       "4  RT @CAOYI170610: According to a recent US CDC ...   \n",
       "\n",
       "                                              source  truncated  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "1  <a href=\"http://twitter.com/#!/download/ipad\" ...      False   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "4  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "\n",
       "   in_reply_to_status_id in_reply_to_status_id_str  in_reply_to_user_id  \\\n",
       "0                    NaN                      None                  NaN   \n",
       "1                    NaN                      None                  NaN   \n",
       "2                    NaN                      None                  NaN   \n",
       "3                    NaN                      None                  NaN   \n",
       "4                    NaN                      None                  NaN   \n",
       "\n",
       "  in_reply_to_user_id_str  ... lang   timestamp_ms quoted_status_id  \\\n",
       "0                    None  ...   en  1607163066397              NaN   \n",
       "1                    None  ...   en  1607163066461              NaN   \n",
       "2                    None  ...   en  1607163066531     1.335162e+18   \n",
       "3                    None  ...   en  1607163066798     1.335132e+18   \n",
       "4                    None  ...   en  1607163066842              NaN   \n",
       "\n",
       "  quoted_status_id_str                                      quoted_status  \\\n",
       "0                  NaN                                                NaN   \n",
       "1                  NaN                                                NaN   \n",
       "2  1335162235216420864  {'created_at': 'Sat Dec 05 10:00:57 +0000 2020...   \n",
       "3  1335132252800503808  {'created_at': 'Sat Dec 05 08:01:49 +0000 2020...   \n",
       "4                  NaN                                                NaN   \n",
       "\n",
       "                             quoted_status_permalink possibly_sensitive  \\\n",
       "0                                                NaN                NaN   \n",
       "1                                                NaN                NaN   \n",
       "2  {'url': 'https://t.co/HrzZRGUtCS', 'expanded':...                NaN   \n",
       "3  {'url': 'https://t.co/738ZoPEBv7', 'expanded':...                NaN   \n",
       "4                                                NaN                NaN   \n",
       "\n",
       "   extended_tweet  display_text_range  extended_entities  \n",
       "0             NaN                 NaN                NaN  \n",
       "1             NaN                 NaN                NaN  \n",
       "2             NaN                 NaN                NaN  \n",
       "3             NaN                 NaN                NaN  \n",
       "4             NaN                 NaN                NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data from output.json scrapped from twitter\n",
    "with open(\"../other-outputs/output.json\", \"rb\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(str_) for str_ in data]\n",
    "\n",
    "df_tweets = pd.DataFrame.from_records(data)\n",
    "\n",
    "# Get unique tweets\n",
    "tweets=df_tweets[df_tweets[\"retweeted_status\"].isna()]\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update (Don't execute this blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(tweets):\n",
    "    \"\"\"\n",
    "    Update number of the tweet's likes and retweed.\n",
    "    \"\"\"\n",
    "    for index, tweet in tweets.iterrows():\n",
    "        try:\n",
    "            tweet = api.get_status(tweet[\"id\"])\n",
    "            tweets.loc[index, \"favorite_count\"] = tweet.favorite_count\n",
    "            tweets.loc[index, \"retweet_count\"] = tweet.retweet_count\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0fd1ac3beb92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "update(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"favorite_count\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../other-outputs/tweets.tsv', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Load data from TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../other-outputs/tweets.tsv\", sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Consult the more likes and more retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most liked tweet: , with a total of 0 likes.\n"
     ]
    }
   ],
   "source": [
    "more_likes = 0\n",
    "text_with_more_likes = \"\"\n",
    "for index, tweet in tweets.iterrows():\n",
    "    likes_tweet = tweet[\"favorite_count\"]\n",
    "    if likes_tweet > more_likes:\n",
    "        more_likes = likes_tweet\n",
    "        text_with_more_likes =  tweet[\"text\"]\n",
    "print(\"Most liked tweet: {}, with a total of {} likes.\".format(text_with_more_likes, more_likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most retweets tweet: , with a total of 0 retweets.\n"
     ]
    }
   ],
   "source": [
    "more_retweets = 0\n",
    "text_with_more_retweets = \"\"\n",
    "for index, tweet in tweets.iterrows():\n",
    "    num_retweets = tweet[\"retweet_count\"]\n",
    "    if num_retweets > more_retweets:\n",
    "        more_retweets = num_retweets\n",
    "        text_with_more_retweets = tweet[\"text\"]\n",
    "        \n",
    "print(\"Most retweets tweet: {}, with a total of {} retweets.\".format(text_with_more_retweets, more_retweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTerms(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    tweet -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    tweet - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "        \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    tweet =  tweet.lower() ## Transform in lowercase\n",
    "    tweet = tweet.replace(\"#\", \"\") ## removing the ‚Äú#‚Äù from the word \n",
    "    tweet =  tweet.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    tweet = [word for word in tweet if word not in stops] #Remove stopwords. \n",
    "    tweet =[stemming.stem(word) for word in tweet] \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverted-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets, numTweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    numTweets -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    IdIndex = {} # dictionary to map page titles to page ids\n",
    "    idf=defaultdict(float) # dictionary with inversed document frequency \n",
    "    \n",
    "    for i,tweet in tweets.iterrows():\n",
    "        tweet_id = i\n",
    "        terms = getTerms(tweet[\"text\"]) #page_title + page_text\n",
    "        userid = int(tweet[\"id\"])            \n",
    "        IdIndex[tweet_id]=userid  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current doc and store it in termdictPage\n",
    "        ## termdictPage ==> { ‚Äòterm1‚Äô: [currentdoc, [list of positions]], ...,‚Äòtermn‚Äô: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##¬†\"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‚Äòweb‚Äô: [1, [0]], ‚Äòretrieval‚Äô: [1, [1,4]], ‚Äòinformation‚Äô: [1, [2]]}\n",
    "        \n",
    "        ## the term ‚Äòweb‚Äô appears in document 1 in positions 0, \n",
    "        ## the term ‚Äòretrieval‚Äô appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (termdictPage)\n",
    "                # append the position to the corrisponding list\n",
    "                \n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "        \n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  \n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]+= 1  # increment df for current term   \n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "         # Compute idf\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(numTweets/df[term])),4)                   \n",
    "                    \n",
    "    return index, tf, df, idf, IdIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 39.57 seconds\n"
     ]
    }
   ],
   "source": [
    "#Generate Index and idIndex. \n",
    "start_time = time.time()\n",
    "numTweets = len(tweets)\n",
    "index, tf, df, idf, idIndex = create_index_tfidf(tweets, numTweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ranking Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(terms, docs, index, idf, tf, idIndex):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    idIndex -- mapping between page id and tweet id\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "                                                                                                                                                                                                                                                                     \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ##¬†Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            if doc in docs:\n",
    "                tweetInfo = tweets[tweets[\"id\"] == idIndex[doc]]\n",
    "                like =  tweetInfo[\"favorite_count\"].values\n",
    "                retweet = tweetInfo[\"retweet_count\"].values\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ idIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocumentsWithLikes(terms, docs, index, idf, tf, idIndex):\n",
    "    \"\"\"\n",
    "    In this function we compute the out ranking score with like and retweets counts. \n",
    "    Because in this case, the amount of rwteets and the number of user's likes\n",
    "    are also one of the important factors for ranking. \n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    idIndex -- mapping between page id and tweet id\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "                                                                                                                                                                                                                                                                     \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ##¬†Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            if doc in docs:\n",
    "                tweetInfo = tweets[tweets[\"id\"] == idIndex[doc]]\n",
    "                like =  tweetInfo[\"favorite_count\"].values\n",
    "                retweet = tweetInfo[\"retweet_count\"].values\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term] + (like + retweet)\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ idIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search with tf-idf ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, idIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "covid\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### My-Score + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocumentsWithLikes(query, docs, index, idf, tf, idIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "covid\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 20 results out of 317 for the seached query:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pelosi bullish on COVID relief: \"We cannot lea...</td>\n",
       "      <td>thehill</td>\n",
       "      <td>Sat Dec 05 10:21:03 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>105</td>\n",
       "      <td>23</td>\n",
       "      <td>http://www.thehill.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a minor example of the kind of short-c...</td>\n",
       "      <td>smolrobots</td>\n",
       "      <td>Sat Dec 05 12:29:54 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.patreon.com/thomasheasmanhunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Vaccine in care homes in the next two weeks\\...</td>\n",
       "      <td>BBCNews</td>\n",
       "      <td>Sat Dec 05 10:12:53 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>63</td>\n",
       "      <td>20</td>\n",
       "      <td>http://www.bbc.co.uk/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üôÑüòèthe new normal~\\nme: yayyyy tested covid neg...</td>\n",
       "      <td>zainaconda</td>\n",
       "      <td>Sat Dec 05 10:18:33 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Plea In Supreme Court Over \"Exorbitant Money\" ...</td>\n",
       "      <td>ndtv</td>\n",
       "      <td>Sat Dec 05 10:21:09 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>http://www.ndtv.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>One of my student work colleagues had to take ...</td>\n",
       "      <td>TheMindsculpter</td>\n",
       "      <td>Sat Dec 05 10:20:14 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>To be fair to fat Joe, he's only done what thi...</td>\n",
       "      <td>ScouseMan_</td>\n",
       "      <td>Sat Dec 05 10:12:34 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Teenager fined ¬£120 after telling police there...</td>\n",
       "      <td>DailyMailUK</td>\n",
       "      <td>Sat Dec 05 10:20:06 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>http://dailymail.co.uk/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>My covid self isolation is over and I can leav...</td>\n",
       "      <td>SpeakSarahSpeak</td>\n",
       "      <td>Sat Dec 05 10:20:19 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.RippleEnergy.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A massive well done and thank you to our #Volu...</td>\n",
       "      <td>CraigAHarman</td>\n",
       "      <td>Sat Dec 05 10:20:42 +0000 2020</td>\n",
       "      <td>[Volunteers]</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>http://www.sja.org.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Are there any stats kept on why people get COV...</td>\n",
       "      <td>conserv8320</td>\n",
       "      <td>Sat Dec 05 10:11:48 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This is like Pfizer, Moderna or AstraZeneca CE...</td>\n",
       "      <td>AcarUmut</td>\n",
       "      <td>Sat Dec 05 10:21:09 +0000 2020</td>\n",
       "      <td>[covid]</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.mfa.gov.tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Finally a #nonleague Saturday of sorts althoug...</td>\n",
       "      <td>NonLeagueCrowd</td>\n",
       "      <td>Sat Dec 05 10:20:30 +0000 2020</td>\n",
       "      <td>[nonleague]</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We #christmas #pimped our COVID sanitising sta...</td>\n",
       "      <td>LasTapasDeLola</td>\n",
       "      <td>Sat Dec 05 10:12:15 +0000 2020</td>\n",
       "      <td>[christmas, pimped, sherry, socialdistancing]</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.lastapasdelola.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>There are 7 ICU patients in my ER tonight. \\n\\...</td>\n",
       "      <td>taylortashjian</td>\n",
       "      <td>Sat Dec 05 10:18:49 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is a sore throat a symptom of covid because......</td>\n",
       "      <td>Vomit_Dragon</td>\n",
       "      <td>Sat Dec 05 10:12:32 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.twitch.tv/vomit_dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Pfizer Covid vaccine is said to cause infe...</td>\n",
       "      <td>Brahmslover1278</td>\n",
       "      <td>Sat Dec 05 10:12:44 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.16personalities.com/infj-strengths...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>COVID spread remains minimal in Texas schools ...</td>\n",
       "      <td>HoustonChron</td>\n",
       "      <td>Sat Dec 05 10:13:03 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>http://houstonchronicle.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jongin said  he was planning TOUR but covid pr...</td>\n",
       "      <td>jadiorkai</td>\n",
       "      <td>Sat Dec 05 10:21:00 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.twitter.com/DAMAGEKAl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>When this covid shite has calmed down I really...</td>\n",
       "      <td>sxphlou</td>\n",
       "      <td>Sat Dec 05 12:29:44 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.sophiedoran.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet         Username  \\\n",
       "0   Pelosi bullish on COVID relief: \"We cannot lea...          thehill   \n",
       "1   This is a minor example of the kind of short-c...       smolrobots   \n",
       "2   - Vaccine in care homes in the next two weeks\\...          BBCNews   \n",
       "3   üôÑüòèthe new normal~\\nme: yayyyy tested covid neg...       zainaconda   \n",
       "4   Plea In Supreme Court Over \"Exorbitant Money\" ...             ndtv   \n",
       "5   One of my student work colleagues had to take ...  TheMindsculpter   \n",
       "6   To be fair to fat Joe, he's only done what thi...       ScouseMan_   \n",
       "7   Teenager fined ¬£120 after telling police there...      DailyMailUK   \n",
       "8   My covid self isolation is over and I can leav...  SpeakSarahSpeak   \n",
       "9   A massive well done and thank you to our #Volu...     CraigAHarman   \n",
       "10  Are there any stats kept on why people get COV...      conserv8320   \n",
       "11  This is like Pfizer, Moderna or AstraZeneca CE...         AcarUmut   \n",
       "12  Finally a #nonleague Saturday of sorts althoug...   NonLeagueCrowd   \n",
       "13  We #christmas #pimped our COVID sanitising sta...   LasTapasDeLola   \n",
       "14  There are 7 ICU patients in my ER tonight. \\n\\...   taylortashjian   \n",
       "15  Is a sore throat a symptom of covid because......     Vomit_Dragon   \n",
       "16  The Pfizer Covid vaccine is said to cause infe...  Brahmslover1278   \n",
       "17  COVID spread remains minimal in Texas schools ...     HoustonChron   \n",
       "18  jongin said  he was planning TOUR but covid pr...        jadiorkai   \n",
       "19  When this covid shite has calmed down I really...          sxphlou   \n",
       "\n",
       "                              Date  \\\n",
       "0   Sat Dec 05 10:21:03 +0000 2020   \n",
       "1   Sat Dec 05 12:29:54 +0000 2020   \n",
       "2   Sat Dec 05 10:12:53 +0000 2020   \n",
       "3   Sat Dec 05 10:18:33 +0000 2020   \n",
       "4   Sat Dec 05 10:21:09 +0000 2020   \n",
       "5   Sat Dec 05 10:20:14 +0000 2020   \n",
       "6   Sat Dec 05 10:12:34 +0000 2020   \n",
       "7   Sat Dec 05 10:20:06 +0000 2020   \n",
       "8   Sat Dec 05 10:20:19 +0000 2020   \n",
       "9   Sat Dec 05 10:20:42 +0000 2020   \n",
       "10  Sat Dec 05 10:11:48 +0000 2020   \n",
       "11  Sat Dec 05 10:21:09 +0000 2020   \n",
       "12  Sat Dec 05 10:20:30 +0000 2020   \n",
       "13  Sat Dec 05 10:12:15 +0000 2020   \n",
       "14  Sat Dec 05 10:18:49 +0000 2020   \n",
       "15  Sat Dec 05 10:12:32 +0000 2020   \n",
       "16  Sat Dec 05 10:12:44 +0000 2020   \n",
       "17  Sat Dec 05 10:13:03 +0000 2020   \n",
       "18  Sat Dec 05 10:21:00 +0000 2020   \n",
       "19  Sat Dec 05 12:29:44 +0000 2020   \n",
       "\n",
       "                                         Hashtags  Likes  Retweets  \\\n",
       "0                                              []    105        23   \n",
       "1                                              []     79         7   \n",
       "2                                              []     63        20   \n",
       "3                                              []     58         3   \n",
       "4                                              []     43         5   \n",
       "5                                              []     26         2   \n",
       "6                                              []     26         0   \n",
       "7                                              []     20         5   \n",
       "8                                              []     20         0   \n",
       "9                                    [Volunteers]     14         2   \n",
       "10                                             []     11         4   \n",
       "11                                        [covid]     10         3   \n",
       "12                                    [nonleague]     12         1   \n",
       "13  [christmas, pimped, sherry, socialdistancing]     13         0   \n",
       "14                                             []     11         2   \n",
       "15                                             []     12         0   \n",
       "16                                             []      8         4   \n",
       "17                                             []      6         4   \n",
       "18                                             []      9         0   \n",
       "19                                             []      8         0   \n",
       "\n",
       "                                                  Url  \n",
       "0                              http://www.thehill.com  \n",
       "1           https://www.patreon.com/thomasheasmanhunt  \n",
       "2                           http://www.bbc.co.uk/news  \n",
       "3                                                None  \n",
       "4                                http://www.ndtv.com/  \n",
       "5                                                None  \n",
       "6                                                None  \n",
       "7                         http://dailymail.co.uk/news  \n",
       "8                         http://www.RippleEnergy.com  \n",
       "9                               http://www.sja.org.uk  \n",
       "10                                               None  \n",
       "11                              http://www.mfa.gov.tr  \n",
       "12                                               None  \n",
       "13                      http://www.lastapasdelola.com  \n",
       "14                                               None  \n",
       "15                 https://www.twitch.tv/vomit_dragon  \n",
       "16  https://www.16personalities.com/infj-strengths...  \n",
       "17                        http://houstonchronicle.com  \n",
       "18                  https://www.twitter.com/DAMAGEKAl  \n",
       "19                       http://www.sophiedoran.co.uk  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "resultTmp = []\n",
    "\n",
    "for d_id in ranked_docs[:top] :\n",
    "    ID = idIndex[d_id]\n",
    "    #print(\"page_id= {} - page_title: {}\".format(d_id, titleIndex[d_id]))\n",
    "    tweetTmp = tweets[tweets[\"id\"] == ID]\n",
    "    tweet = tweetTmp[\"text\"].values[0]\n",
    "    username = ast.literal_eval(tweetTmp[\"user\"].values[0])[\"screen_name\"]\n",
    "    date = tweetTmp[\"created_at\"].values[0]\n",
    "    hashtags_lists = ast.literal_eval(tweetTmp[\"entities\"].values[0])[\"hashtags\"]\n",
    "    hashtags = [tag[\"text\"] for tag in hashtags_lists]\n",
    "    likes = tweetTmp[\"favorite_count\"].values[0]\n",
    "    retweets = tweetTmp[\"retweet_count\"].values[0]\n",
    "    url = ast.literal_eval(tweetTmp[\"user\"].values[0])[\"url\"]\n",
    "    \n",
    "    resultTmp.append([tweet,username,date,hashtags,likes,retweets,url])\n",
    "    \n",
    "result = pd.DataFrame(resultTmp,columns=[\"Tweet\", \"Username\", \"Date\", \"Hashtags\", \"Likes\", \"Retweets\", \"Url\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docs, preprocess=preprocess_string, verbose=10000):\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        yield preprocess(doc)#  preprocess\n",
    "        \n",
    "        # print progress if needed\n",
    "        if verbose > 0 and (i + 1) % verbose == 0:\n",
    "            print(f\"Progress: {i + 1}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
