{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wenjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## access token informations \n",
    "access_token1 = \"1240651902712516613-BF3CepSxPUoMr3lDkptNt08f2GwaGZ\"\n",
    "access_token_secret1 = \"79ga9EksEgtgpHGJhXy7A4ujb31pw1AjwP8lOlaYtmDdk\"\n",
    "\n",
    "consumer_key1 = \"ieusj8Wof83kmsVe9PDLJQqn3\"\n",
    "consumer_secret1 = \"EpZYsMfEqvHSwFQyAfz2wCdVujPTtr7JQtvkBt1g9sMg5t0m3B\"\n",
    "\n",
    "#Generate API\n",
    "auth = OAuthHandler(consumer_key1, consumer_secret1)\n",
    "auth.set_access_token(access_token1, access_token_secret1)\n",
    "api = API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Load data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from output.json scrapped from twitter\n",
    "with open(\"../other-outputs/output.json\", \"rb\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(str_) for str_ in data]\n",
    "\n",
    "df_tweets = pd.DataFrame.from_records(data)\n",
    "\n",
    "# Get unique tweets\n",
    "tweets=df_tweets[df_tweets[\"retweeted_status\"].isna()]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Print information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_retweets_uniqueTweets_uniqueUsers(df_tweets):\n",
    "    retweets_ = df_tweets[\"retweeted_status\"].apply(lambda x: 0 if str(x) == \"nan\" else 1)\n",
    "    tot_retweets = sum(retweets_)\n",
    "    unique_tweets = len(retweets_) - tot_retweets\n",
    "    \n",
    "    users_ = df_tweets.user.apply(lambda x: x[\"id\"])\n",
    "    \n",
    "    tot_users = len(set(users_))\n",
    "    \n",
    "    print(\"Total Retweets:\", tot_retweets)\n",
    "\n",
    "    print(\"Unique Tweets:\", unique_tweets)\n",
    "    \n",
    "    print(\"Unique Users:\", tot_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_retweets_uniqueTweets_uniqueUsers(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Update (Don't execute this blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(tweets):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Update number of the tweet's likes and retweed.\n",
    "    \"\"\"\n",
    "    for index, tweet in tweets.iterrows():\n",
    "        try:\n",
    "            tweet = api.get_status(tweet[\"id\"])\n",
    "            tweets.loc[index, \"favorite_count\"] = tweet.favorite_count\n",
    "            tweets.loc[index, \"retweet_count\"] = tweet.retweet_count\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"favorite_count\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../other-outputs/tweets.tsv', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  4. Load data from TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../other-outputs/tweets.tsv\", sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. Consult the more likes and more retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most liked tweet: When a former VP of Pfizer is calling for the immediate suspension of all SARS CoV 2 vaccine studies, because of th… https://t.co/s6yvRBZKoh, with a total of 171 likes.\n"
     ]
    }
   ],
   "source": [
    "more_likes = 0\n",
    "text_with_more_likes = \"\"\n",
    "for index, tweet in tweets.iterrows():\n",
    "    likes_tweet = tweet[\"favorite_count\"]\n",
    "    if likes_tweet > more_likes:\n",
    "        more_likes = likes_tweet\n",
    "        text_with_more_likes =  tweet[\"text\"]\n",
    "print(\"Most liked tweet: {}, with a total of {} likes.\".format(text_with_more_likes, more_likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most retweets tweet: When a former VP of Pfizer is calling for the immediate suspension of all SARS CoV 2 vaccine studies, because of th… https://t.co/s6yvRBZKoh, with a total of 104 retweets.\n"
     ]
    }
   ],
   "source": [
    "more_retweets = 0\n",
    "text_with_more_retweets = \"\"\n",
    "for index, tweet in tweets.iterrows():\n",
    "    num_retweets = tweet[\"retweet_count\"]\n",
    "    if num_retweets > more_retweets:\n",
    "        more_retweets = num_retweets\n",
    "        text_with_more_retweets = tweet[\"text\"]\n",
    "        \n",
    "print(\"Most retweets tweet: {}, with a total of {} retweets.\".format(text_with_more_retweets, more_retweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Pre-processing the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTerms(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    tweet -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    tweet - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "        \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    tweet =  tweet.lower() ## Transform in lowercase\n",
    "    tweet = tweet.replace(\"#\", \"\") ## removing the “#” from the word \n",
    "    tweet =  tweet.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    tweet = [word for word in tweet if word not in stops] #Remove stopwords. \n",
    "    tweet =[stemming.stem(word) for word in tweet] \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Inverted-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets, numTweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    numTweets -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    IdIndex = {} # dictionary to map page titles to page ids\n",
    "    idf=defaultdict(float) # dictionary with inversed document frequency \n",
    "    \n",
    "    for i,tweet in tweets.iterrows():\n",
    "        tweet_id = i\n",
    "        terms = getTerms(tweet[\"text\"]) #page_title + page_text\n",
    "        userid = int(tweet[\"id\"])            \n",
    "        IdIndex[tweet_id]=userid  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current doc and store it in termdictPage\n",
    "        ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ## \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        \n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (termdictPage)\n",
    "                # append the position to the corrisponding list\n",
    "                \n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "        \n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  \n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]+= 1  # increment df for current term   \n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "         # Compute idf\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(numTweets/df[term])),4)                   \n",
    "                    \n",
    "    return index, tf, df, idf, IdIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 43.68 seconds\n"
     ]
    }
   ],
   "source": [
    "#Generate Index and idIndex. \n",
    "start_time = time.time()\n",
    "numTweets = len(tweets)\n",
    "index, tf, df, idf, idIndex = create_index_tfidf(tweets, numTweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. output search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_query(ranked_docs): \n",
    "    \"\"\"\n",
    "        This functin is used to print the result. \n",
    "    \"\"\"\n",
    "    top = 20\n",
    "\n",
    "    print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "    resultTmp = []\n",
    "\n",
    "    for d_id in ranked_docs[:top] :\n",
    "        ID = idIndex[d_id]\n",
    "        #print(\"page_id= {} - page_title: {}\".format(d_id, titleIndex[d_id]))\n",
    "        tweetTmp = tweets[tweets[\"id\"] == ID]\n",
    "        tweet = tweetTmp[\"text\"].values[0]\n",
    "        username = ast.literal_eval(tweetTmp[\"user\"].values[0])[\"screen_name\"]\n",
    "        date = tweetTmp[\"created_at\"].values[0]\n",
    "        hashtags_lists = ast.literal_eval(tweetTmp[\"entities\"].values[0])[\"hashtags\"]\n",
    "        hashtags = [tag[\"text\"] for tag in hashtags_lists]\n",
    "        likes = tweetTmp[\"favorite_count\"].values[0]\n",
    "        retweets = tweetTmp[\"retweet_count\"].values[0]\n",
    "        url = ast.literal_eval(tweetTmp[\"user\"].values[0])[\"url\"]\n",
    "\n",
    "        resultTmp.append([tweet,username,date,hashtags,likes,retweets,url])\n",
    "\n",
    "    result = pd.DataFrame(resultTmp,columns=[\"Tweet\", \"Username\", \"Date\", \"Hashtags\", \"Likes\", \"Retweets\", \"Url\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Ranking Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(terms, docs, index, idf, tf, idIndex):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    idIndex -- mapping between page id and tweet id\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "                                                                                                                                                                                                                                                                     \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            if doc in docs:\n",
    "                tweetInfo = tweets[tweets[\"id\"] == idIndex[doc]]\n",
    "                like =  tweetInfo[\"favorite_count\"].values\n",
    "                retweet = tweetInfo[\"retweet_count\"].values\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ idIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Search with tf-idf ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, idIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "covid-19 america\n",
      "\n",
      "======================\n",
      "Top 20 results out of 155 for the seached query:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lock them all up they are responsible for the ...</td>\n",
       "      <td>madog190157</td>\n",
       "      <td>Sat Dec 05 10:13:18 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@WillyMasterson @JoeBiden Everyone in America ...</td>\n",
       "      <td>Johnsontb1</td>\n",
       "      <td>Sat Dec 05 10:22:01 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reading Covid discourse from America on Twitte...</td>\n",
       "      <td>JamieCPitman</td>\n",
       "      <td>Sat Dec 05 12:29:49 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://Instagram.com/jcpitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@chris_herd I’m witnessing it now. My concern ...</td>\n",
       "      <td>NakoMbelle</td>\n",
       "      <td>Sat Dec 05 10:21:15 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.fintechrecruiters.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After shed loads of information warnings &amp;amp;...</td>\n",
       "      <td>alanjoh1949</td>\n",
       "      <td>Sat Dec 05 10:11:10 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@lack__lustre Covid-19</td>\n",
       "      <td>RanaTal80613033</td>\n",
       "      <td>Sat Dec 05 10:21:33 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What have you experienced with this Covid-19 p...</td>\n",
       "      <td>misstmoraa</td>\n",
       "      <td>Sat Dec 05 10:20:54 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As well did COVID-19 Deaths https://t.co/kX8Hw...</td>\n",
       "      <td>ParodyTheodore</td>\n",
       "      <td>Sat Dec 05 10:22:04 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Have Covid-19 passport, will travel https://t....</td>\n",
       "      <td>BusinessTimes</td>\n",
       "      <td>Sat Dec 05 10:21:13 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>COVID-19 Malaysia Updates https://t.co/9aSK7l1lYz</td>\n",
       "      <td>HowKokKeng1</td>\n",
       "      <td>Sat Dec 05 10:13:18 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Asymptomatic People Do NOT Spread COVID-19 htt...</td>\n",
       "      <td>Vman02605333</td>\n",
       "      <td>Sat Dec 05 10:13:32 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.magapill.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@AngieJo63181310 @JoeBiden The White House is ...</td>\n",
       "      <td>Chanucka2</td>\n",
       "      <td>Sat Dec 05 12:29:44 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Telehealth usage soars during COVID-19 https:/...</td>\n",
       "      <td>BackboneUK</td>\n",
       "      <td>Sat Dec 05 10:13:33 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.backbone.uk.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@anilvijminister Covid-19 is not pendamic it's...</td>\n",
       "      <td>UttamCh35735268</td>\n",
       "      <td>Sat Dec 05 10:11:36 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>United States faces dramatic worsening of Covi...</td>\n",
       "      <td>nobodytweetnob</td>\n",
       "      <td>Sat Dec 05 10:13:40 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#LatestNews #coronavirus Covid-19 &amp;amp; Mortga...</td>\n",
       "      <td>mortgage_guru</td>\n",
       "      <td>Sat Dec 05 10:13:32 +0000 2020</td>\n",
       "      <td>[LatestNews, coronavirus]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Pakistan records over 3,000 Covid-19 cases for...</td>\n",
       "      <td>MouliNalagatla</td>\n",
       "      <td>Sat Dec 05 10:12:14 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Petition: Prevent any restrictions on those wh...</td>\n",
       "      <td>fsprops96</td>\n",
       "      <td>Sat Dec 05 10:11:11 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.fsproperties.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Moscow Starts Mass COVID-19 Vaccination With I...</td>\n",
       "      <td>Dailynews4meCom</td>\n",
       "      <td>Sat Dec 05 10:22:09 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://dailynews4me.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@GavinWax @NYYRC This is the party where Covid...</td>\n",
       "      <td>Rez_AmongUs</td>\n",
       "      <td>Sat Dec 05 10:21:22 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet         Username  \\\n",
       "0   Lock them all up they are responsible for the ...      madog190157   \n",
       "1   @WillyMasterson @JoeBiden Everyone in America ...       Johnsontb1   \n",
       "2   Reading Covid discourse from America on Twitte...     JamieCPitman   \n",
       "3   @chris_herd I’m witnessing it now. My concern ...       NakoMbelle   \n",
       "4   After shed loads of information warnings &amp;...      alanjoh1949   \n",
       "5                              @lack__lustre Covid-19  RanaTal80613033   \n",
       "6   What have you experienced with this Covid-19 p...       misstmoraa   \n",
       "7   As well did COVID-19 Deaths https://t.co/kX8Hw...   ParodyTheodore   \n",
       "8   Have Covid-19 passport, will travel https://t....    BusinessTimes   \n",
       "9   COVID-19 Malaysia Updates https://t.co/9aSK7l1lYz      HowKokKeng1   \n",
       "10  Asymptomatic People Do NOT Spread COVID-19 htt...     Vman02605333   \n",
       "11  @AngieJo63181310 @JoeBiden The White House is ...        Chanucka2   \n",
       "12  Telehealth usage soars during COVID-19 https:/...       BackboneUK   \n",
       "13  @anilvijminister Covid-19 is not pendamic it's...  UttamCh35735268   \n",
       "14  United States faces dramatic worsening of Covi...   nobodytweetnob   \n",
       "15  #LatestNews #coronavirus Covid-19 &amp; Mortga...    mortgage_guru   \n",
       "16  Pakistan records over 3,000 Covid-19 cases for...   MouliNalagatla   \n",
       "17  Petition: Prevent any restrictions on those wh...        fsprops96   \n",
       "18  Moscow Starts Mass COVID-19 Vaccination With I...  Dailynews4meCom   \n",
       "19  @GavinWax @NYYRC This is the party where Covid...      Rez_AmongUs   \n",
       "\n",
       "                              Date                   Hashtags  Likes  \\\n",
       "0   Sat Dec 05 10:13:18 +0000 2020                         []      1   \n",
       "1   Sat Dec 05 10:22:01 +0000 2020                         []      0   \n",
       "2   Sat Dec 05 12:29:49 +0000 2020                         []      0   \n",
       "3   Sat Dec 05 10:21:15 +0000 2020                         []      1   \n",
       "4   Sat Dec 05 10:11:10 +0000 2020                         []      4   \n",
       "5   Sat Dec 05 10:21:33 +0000 2020                         []      1   \n",
       "6   Sat Dec 05 10:20:54 +0000 2020                         []      0   \n",
       "7   Sat Dec 05 10:22:04 +0000 2020                         []      3   \n",
       "8   Sat Dec 05 10:21:13 +0000 2020                         []      0   \n",
       "9   Sat Dec 05 10:13:18 +0000 2020                         []      0   \n",
       "10  Sat Dec 05 10:13:32 +0000 2020                         []      0   \n",
       "11  Sat Dec 05 12:29:44 +0000 2020                         []      1   \n",
       "12  Sat Dec 05 10:13:33 +0000 2020                         []      0   \n",
       "13  Sat Dec 05 10:11:36 +0000 2020                         []      0   \n",
       "14  Sat Dec 05 10:13:40 +0000 2020                         []      0   \n",
       "15  Sat Dec 05 10:13:32 +0000 2020  [LatestNews, coronavirus]      0   \n",
       "16  Sat Dec 05 10:12:14 +0000 2020                         []      0   \n",
       "17  Sat Dec 05 10:11:11 +0000 2020                         []      0   \n",
       "18  Sat Dec 05 10:22:09 +0000 2020                         []      0   \n",
       "19  Sat Dec 05 10:21:22 +0000 2020                         []      0   \n",
       "\n",
       "    Retweets                               Url  \n",
       "0          0                              None  \n",
       "1          0                              None  \n",
       "2          0     http://Instagram.com/jcpitman  \n",
       "3          0  http://www.fintechrecruiters.com  \n",
       "4          1                              None  \n",
       "5          0                              None  \n",
       "6          0                              None  \n",
       "7          0                              None  \n",
       "8          0                              None  \n",
       "9          0                              None  \n",
       "10         0          http://www.magapill.com/  \n",
       "11         0                              None  \n",
       "12         1        http://www.backbone.uk.com  \n",
       "13         0                              None  \n",
       "14         0                              None  \n",
       "15         0                              None  \n",
       "16         0                              None  \n",
       "17         0     http://www.fsproperties.co.uk  \n",
       "18         0         https://dailynews4me.com/  \n",
       "19         0                              None  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "\n",
    "output_query(ranked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. My score Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocumentsWithLikes(terms, docs, index, idf, tf, idIndex):\n",
    "    \"\"\"\n",
    "    In this function we compute the out ranking score with like and retweets counts. \n",
    "    Because in this case, the amount of rwteets and the number of user's likes\n",
    "    are also one of the important factors for ranking. \n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    idIndex -- mapping between page id and tweet id\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "                                                                                                                                                                                                                                                                     \n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            if doc in docs:\n",
    "                tweetInfo = tweets[tweets[\"id\"] == idIndex[doc]]\n",
    "                like =  tweetInfo[\"favorite_count\"].values\n",
    "                retweet = tweetInfo[\"retweet_count\"].values\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term] + (like + retweet)\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(np.squeeze(curDocVec), np.squeeze(queryVector)), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ idIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11. My-Score + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_score(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocumentsWithLikes(query, docs, index, idf, tf, idIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "coronavirus death rate barcelona\n",
      "\n",
      "======================\n",
      "Top 20 results out of 122 for the seached query:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which part? \\n\\nThe virus, which is part of th...</td>\n",
       "      <td>mynameisjerm</td>\n",
       "      <td>Sat Dec 05 12:29:46 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>https://jermwarfare.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The man behind Sweden’s coronavirus strategy, ...</td>\n",
       "      <td>business</td>\n",
       "      <td>Sat Dec 05 10:20:05 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>http://www.bloomberg.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@MichaelYeadon3 Please share with everyone\\n\\n...</td>\n",
       "      <td>lyne_ian</td>\n",
       "      <td>Sat Dec 05 10:20:22 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whenever this coronavirus shit ends I’m going ...</td>\n",
       "      <td>bloodyfaceHS</td>\n",
       "      <td>Sat Dec 05 10:12:13 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>http://twitch.tv/bloodyfacehs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@GerardBattenUK Please share with everyone\\n\\n...</td>\n",
       "      <td>lyne_ian</td>\n",
       "      <td>Sat Dec 05 10:21:41 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MoscowMitch has been the cause of thousands of...</td>\n",
       "      <td>RoyHay7</td>\n",
       "      <td>Sat Dec 05 10:20:10 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pakistan reports 44 deaths by Covid-19 and 3,1...</td>\n",
       "      <td>LeAnam__</td>\n",
       "      <td>Sat Dec 05 10:21:04 +0000 2020</td>\n",
       "      <td>[COVIDSecondWave]</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>http://Instagram.com/anamblogga_/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Health minister tests positive after getting c...</td>\n",
       "      <td>NigeriaNewsdesk</td>\n",
       "      <td>Sat Dec 05 10:22:04 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UK hit by worst economic hit then anyone else ...</td>\n",
       "      <td>mmaher70</td>\n",
       "      <td>Sat Dec 05 10:20:55 +0000 2020</td>\n",
       "      <td>[coronavirus]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Last week, Dr. Jonathan Reiner of George Washi...</td>\n",
       "      <td>WoodwarddianneJ</td>\n",
       "      <td>Sat Dec 05 10:12:23 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@Nigel_Farage Even Conservative MPs are public...</td>\n",
       "      <td>jeremy_hume</td>\n",
       "      <td>Sat Dec 05 10:12:58 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sustainable transport &amp;amp; greening city spac...</td>\n",
       "      <td>eimear_vance</td>\n",
       "      <td>Sat Dec 05 10:22:12 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The latest update of #Coronavirus (#COVID19) i...</td>\n",
       "      <td>DHA_Dubai</td>\n",
       "      <td>Sat Dec 05 10:21:04 +0000 2020</td>\n",
       "      <td>[Coronavirus, COVID19, UAE, We_Are_All_Respons...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>http://www.dha.gov.ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Russia’s coronavirus death toll rises to 42,68...</td>\n",
       "      <td>IlkhaAgency</td>\n",
       "      <td>Sat Dec 05 10:13:00 +0000 2020</td>\n",
       "      <td>[CoronavirusPandemic, BillGates, CoronaVaccine]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>https://ilkha.com/english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Finding it tough to keep up with the latest co...</td>\n",
       "      <td>Autism</td>\n",
       "      <td>Sat Dec 05 14:58:01 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>http://bit.ly/NationalAutisticSociety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MOSCOW: Moscow on Saturday began vaccinating w...</td>\n",
       "      <td>BangkokPostNews</td>\n",
       "      <td>Sat Dec 05 10:12:54 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.bangkokpost.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>South Korea urges vigilance as Covid-19 cluste...</td>\n",
       "      <td>dawn_com</td>\n",
       "      <td>Sat Dec 05 10:21:56 +0000 2020</td>\n",
       "      <td>[coronavirus]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.dawn.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VA plans for #vaccine distribution unclear as ...</td>\n",
       "      <td>DAVHQ</td>\n",
       "      <td>Sat Dec 05 14:58:00 +0000 2020</td>\n",
       "      <td>[vaccine, coronavirus]</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.dav.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Europe’s drug regulator, the EMA has opposed t...</td>\n",
       "      <td>BremainInSpain</td>\n",
       "      <td>Sat Dec 05 10:12:52 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.bremaininspain.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Coronavirus trims strength of Hindu jatha’s go...</td>\n",
       "      <td>TOIIndiaNews</td>\n",
       "      <td>Sat Dec 05 10:20:45 +0000 2020</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>http://timesofindia.indiatimes.com/india</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet         Username  \\\n",
       "0   Which part? \\n\\nThe virus, which is part of th...     mynameisjerm   \n",
       "1   The man behind Sweden’s coronavirus strategy, ...         business   \n",
       "2   @MichaelYeadon3 Please share with everyone\\n\\n...         lyne_ian   \n",
       "3   Whenever this coronavirus shit ends I’m going ...     bloodyfaceHS   \n",
       "4   @GerardBattenUK Please share with everyone\\n\\n...         lyne_ian   \n",
       "5   MoscowMitch has been the cause of thousands of...          RoyHay7   \n",
       "6   Pakistan reports 44 deaths by Covid-19 and 3,1...         LeAnam__   \n",
       "7   Health minister tests positive after getting c...  NigeriaNewsdesk   \n",
       "8   UK hit by worst economic hit then anyone else ...         mmaher70   \n",
       "9   Last week, Dr. Jonathan Reiner of George Washi...  WoodwarddianneJ   \n",
       "10  @Nigel_Farage Even Conservative MPs are public...      jeremy_hume   \n",
       "11  Sustainable transport &amp; greening city spac...     eimear_vance   \n",
       "12  The latest update of #Coronavirus (#COVID19) i...        DHA_Dubai   \n",
       "13  Russia’s coronavirus death toll rises to 42,68...      IlkhaAgency   \n",
       "14  Finding it tough to keep up with the latest co...           Autism   \n",
       "15  MOSCOW: Moscow on Saturday began vaccinating w...  BangkokPostNews   \n",
       "16  South Korea urges vigilance as Covid-19 cluste...         dawn_com   \n",
       "17  VA plans for #vaccine distribution unclear as ...            DAVHQ   \n",
       "18  Europe’s drug regulator, the EMA has opposed t...   BremainInSpain   \n",
       "19  Coronavirus trims strength of Hindu jatha’s go...     TOIIndiaNews   \n",
       "\n",
       "                              Date  \\\n",
       "0   Sat Dec 05 12:29:46 +0000 2020   \n",
       "1   Sat Dec 05 10:20:05 +0000 2020   \n",
       "2   Sat Dec 05 10:20:22 +0000 2020   \n",
       "3   Sat Dec 05 10:12:13 +0000 2020   \n",
       "4   Sat Dec 05 10:21:41 +0000 2020   \n",
       "5   Sat Dec 05 10:20:10 +0000 2020   \n",
       "6   Sat Dec 05 10:21:04 +0000 2020   \n",
       "7   Sat Dec 05 10:22:04 +0000 2020   \n",
       "8   Sat Dec 05 10:20:55 +0000 2020   \n",
       "9   Sat Dec 05 10:12:23 +0000 2020   \n",
       "10  Sat Dec 05 10:12:58 +0000 2020   \n",
       "11  Sat Dec 05 10:22:12 +0000 2020   \n",
       "12  Sat Dec 05 10:21:04 +0000 2020   \n",
       "13  Sat Dec 05 10:13:00 +0000 2020   \n",
       "14  Sat Dec 05 14:58:01 +0000 2020   \n",
       "15  Sat Dec 05 10:12:54 +0000 2020   \n",
       "16  Sat Dec 05 10:21:56 +0000 2020   \n",
       "17  Sat Dec 05 14:58:00 +0000 2020   \n",
       "18  Sat Dec 05 10:12:52 +0000 2020   \n",
       "19  Sat Dec 05 10:20:45 +0000 2020   \n",
       "\n",
       "                                             Hashtags  Likes  Retweets  \\\n",
       "0                                                  []     75         7   \n",
       "1                                                  []     40        13   \n",
       "2                                                  []     21         8   \n",
       "3                                                  []     24         0   \n",
       "4                                                  []     11        11   \n",
       "5                                                  []     10         4   \n",
       "6                                   [COVIDSecondWave]     14         0   \n",
       "7                                                  []      7        10   \n",
       "8                                       [coronavirus]      4         3   \n",
       "9                                                  []      3         2   \n",
       "10                                                 []     13         2   \n",
       "11                                                 []      3         0   \n",
       "12  [Coronavirus, COVID19, UAE, We_Are_All_Respons...      4         5   \n",
       "13    [CoronavirusPandemic, BillGates, CoronaVaccine]      2         1   \n",
       "14                                                 []      7         1   \n",
       "15                                                 []      6         1   \n",
       "16                                      [coronavirus]      6         1   \n",
       "17                             [vaccine, coronavirus]      5         1   \n",
       "18                                                 []      3         3   \n",
       "19                                                 []      3         2   \n",
       "\n",
       "                                         Url  \n",
       "0                    https://jermwarfare.com  \n",
       "1                   http://www.bloomberg.com  \n",
       "2                                       None  \n",
       "3              http://twitch.tv/bloodyfacehs  \n",
       "4                                       None  \n",
       "5                                       None  \n",
       "6          http://Instagram.com/anamblogga_/  \n",
       "7                                       None  \n",
       "8                                       None  \n",
       "9                                       None  \n",
       "10                                      None  \n",
       "11                                      None  \n",
       "12                     http://www.dha.gov.ae  \n",
       "13                 https://ilkha.com/english  \n",
       "14     http://bit.ly/NationalAutisticSociety  \n",
       "15                http://www.bangkokpost.com  \n",
       "16                       http://www.dawn.com  \n",
       "17                      https://www.dav.org/  \n",
       "18             http://www.bremaininspain.com  \n",
       "19  http://timesofindia.indiatimes.com/india  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = my_score(query, index)    \n",
    "output_query(ranked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = {}\n",
    "for text in df_tweets[\"text\"]:\n",
    "    \n",
    "    # remove \"RT\" string indicating a retweet\n",
    "    text = text.replace(\"RT \", \"\").strip()\n",
    "    \n",
    "    # lowering text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing all the punctuations\n",
    "    text = re.sub(r'[^\\w\\s]','',text).strip()\n",
    "    \n",
    "    # tokenize the text\n",
    "    lst_text = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    lst_text = [x for x in lst_text if x not in STOPWORDS]\n",
    "    \n",
    "        \n",
    "    # create bag-of-words - for each word the frequency of the word in the corpus\n",
    "    for w in lst_text:\n",
    "        if w not in bag_of_words:\n",
    "            bag_of_words[w] = 0\n",
    "        bag_of_words[w] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(title, dic_):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(18,7))\n",
    "    wordcloud = WordCloud(background_color=\"white\",width=1600, height=800)\n",
    "    wordcloud = wordcloud.generate_from_frequencies(dic_)\n",
    "    ax.axis(\"off\")     \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(\"WordCloud - All Tweets\", bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot of the 10 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    hashtags_lists = df_tweets[\"entities\"].apply(lambda x: x[\"hashtags\"])\n",
    "    \n",
    "    hashtags = hashtags_lists.apply(lambda x: x[0][\"text\"].lower() if x != [] else None)\n",
    "    \n",
    "    hashtags_by_frequency = Counter(hashtags)\n",
    "    hashtags_by_frequency = {k: hashtags_by_frequency[k] for k in hashtags_by_frequency if k != None} \n",
    "    \n",
    "    return hashtags_by_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_by_frequency = extract_hashtags()\n",
    "hashtags_by_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags = pd.DataFrame(hashtags_by_frequency.items())\n",
    "df_hashtags.columns = [\"hashtag\", \"count\"]\n",
    "df_hashtags.set_index(\"hashtag\", inplace=True)\n",
    "df_hashtags.sort_values(\"count\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags.head(10).plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docs, preprocess=preprocess_string, verbose=10000):\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        yield preprocess(doc)#  preprocess\n",
    "        \n",
    "        # print progress if needed\n",
    "        if verbose > 0 and (i + 1) % verbose == 0:\n",
    "            print(f\"Progress: {i + 1}\")\n",
    "    return docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
